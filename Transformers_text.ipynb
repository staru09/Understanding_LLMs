{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import pipeline\n\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\ntranslator(\"Ce cours est produit par Hugging Face.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:11:48.760183Z","iopub.execute_input":"2024-07-20T13:11:48.760959Z","iopub.status.idle":"2024-07-20T13:12:31.589646Z","shell.execute_reply.started":"2024-07-20T13:11:48.760912Z","shell.execute_reply":"2024-07-20T13:12:31.587972Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-20 13:12:00.537674: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-20 13:12:00.537778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-20 13:12:00.822176: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9755d67f7b8847a08041d942ebc18959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79eba42d24c3418da4a2d1e03aacebe8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e85c42e4f54dc9a3e4fabb357c9b8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dcb9b7e43e84697b9118d644e9b7efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0f28021eb54fa8a91daf57a5224c01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8747e492dcd546d0bccd71aa6b8fc027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e07e6c64ef14e12bc80555e8d7b6ee3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"[{'translation_text': 'This course is produced by Hugging Face.'}]"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\nunmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\nresult = unmasker(\"This man works as a [MASK].\")\nprint([r[\"token_str\"] for r in result])\n\nresult = unmasker(\"This woman works as a [MASK].\")\nprint([r[\"token_str\"] for r in result])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:12:31.591407Z","iopub.execute_input":"2024-07-20T13:12:31.592182Z","iopub.status.idle":"2024-07-20T13:12:38.941192Z","shell.execute_reply.started":"2024-07-20T13:12:31.592146Z","shell.execute_reply":"2024-07-20T13:12:38.940236Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f53baddf3144b98a5864b08781a1290"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b532a4699f0d4a3ca9016a98186ac3af"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62381c371cce424dbedd691db08a0a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24f555200a642a7a186d26cddcfb658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90054266670471eb44068860b7f1417"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\nner = pipeline(\"ner\", grouped_entities=True)\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:12:38.944293Z","iopub.execute_input":"2024-07-20T13:12:38.944598Z","iopub.status.idle":"2024-07-20T13:13:03.088747Z","shell.execute_reply.started":"2024-07-20T13:12:38.944571Z","shell.execute_reply":"2024-07-20T13:13:03.087801Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfec6e7845d7483d961234b57bba9980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"544d13c6f93347e6944efef81bfe2d81"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec46164670c4612a51510ecfabde2b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5595d10654aa4917b30f1fd9b9afc282"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[{'entity_group': 'PER',\n  'score': 0.9981694,\n  'word': 'Sylvain',\n  'start': 11,\n  'end': 18},\n {'entity_group': 'ORG',\n  'score': 0.9796019,\n  'word': 'Hugging Face',\n  'start': 33,\n  'end': 45},\n {'entity_group': 'LOC',\n  'score': 0.9932106,\n  'word': 'Brooklyn',\n  'start': 49,\n  'end': 57}]"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:03.091430Z","iopub.execute_input":"2024-07-20T13:13:03.092029Z","iopub.status.idle":"2024-07-20T13:13:03.950685Z","shell.execute_reply.started":"2024-07-20T13:13:03.091971Z","shell.execute_reply":"2024-07-20T13:13:03.949683Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed95f5b19e434a13a8ff21a17cfd1146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb6dbbf996d416cb5f1447ed3ee7b9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693e0c62adb048b6b966ec5ac12d2529"}},"metadata":{}}]},{"cell_type":"code","source":"raw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n    \"That could be an irony\"\n]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:03.951836Z","iopub.execute_input":"2024-07-20T13:13:03.952172Z","iopub.status.idle":"2024-07-20T13:13:03.961351Z","shell.execute_reply.started":"2024-07-20T13:13:03.952135Z","shell.execute_reply":"2024-07-20T13:13:03.959247Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  101,  2008,  2071,  2022,  2019, 19728,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModel\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModel.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:03.962646Z","iopub.execute_input":"2024-07-20T13:13:03.962977Z","iopub.status.idle":"2024-07-20T13:13:10.441200Z","shell.execute_reply.started":"2024-07-20T13:13:03.962946Z","shell.execute_reply":"2024-07-20T13:13:10.440327Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17bbe3f2e2c04a49a5e198c0acb6af24"}},"metadata":{}}]},{"cell_type":"code","source":"outputs = model(**inputs)\nprint(outputs.last_hidden_state.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:10.442334Z","iopub.execute_input":"2024-07-20T13:13:10.444515Z","iopub.status.idle":"2024-07-20T13:13:10.508808Z","shell.execute_reply.started":"2024-07-20T13:13:10.444487Z","shell.execute_reply":"2024-07-20T13:13:10.507856Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"torch.Size([3, 16, 768])\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs = model(**inputs)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:10.509827Z","iopub.execute_input":"2024-07-20T13:13:10.510159Z","iopub.status.idle":"2024-07-20T13:13:15.460975Z","shell.execute_reply.started":"2024-07-20T13:13:10.510132Z","shell.execute_reply":"2024-07-20T13:13:15.460190Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print(outputs.logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:15.462105Z","iopub.execute_input":"2024-07-20T13:13:15.462402Z","iopub.status.idle":"2024-07-20T13:13:15.476127Z","shell.execute_reply.started":"2024-07-20T13:13:15.462376Z","shell.execute_reply":"2024-07-20T13:13:15.475354Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([[-1.5607,  1.6123],\n        [ 4.1692, -3.3464],\n        [ 1.5989, -1.3391]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:15.479351Z","iopub.execute_input":"2024-07-20T13:13:15.479621Z","iopub.status.idle":"2024-07-20T13:13:15.485211Z","shell.execute_reply.started":"2024-07-20T13:13:15.479597Z","shell.execute_reply":"2024-07-20T13:13:15.484321Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tensor([[4.0195e-02, 9.5980e-01],\n        [9.9946e-01, 5.4418e-04],\n        [9.4970e-01, 5.0303e-02]], grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.id2label","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:15.486310Z","iopub.execute_input":"2024-07-20T13:13:15.486627Z","iopub.status.idle":"2024-07-20T13:13:15.498090Z","shell.execute_reply.started":"2024-07-20T13:13:15.486595Z","shell.execute_reply":"2024-07-20T13:13:15.497352Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{0: 'NEGATIVE', 1: 'POSITIVE'}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertConfig, BertModel\n\n# Building the config\nconfig = BertConfig()\n\n# Building the model from the config\nmodel = BertModel(config)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:15.498967Z","iopub.execute_input":"2024-07-20T13:13:15.499239Z","iopub.status.idle":"2024-07-20T13:13:17.405508Z","shell.execute_reply.started":"2024-07-20T13:13:15.499217Z","shell.execute_reply":"2024-07-20T13:13:17.404681Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(config)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:17.406620Z","iopub.execute_input":"2024-07-20T13:13:17.406910Z","iopub.status.idle":"2024-07-20T13:13:17.412684Z","shell.execute_reply.started":"2024-07-20T13:13:17.406884Z","shell.execute_reply":"2024-07-20T13:13:17.411783Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.42.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:17.413910Z","iopub.execute_input":"2024-07-20T13:13:17.414310Z","iopub.status.idle":"2024-07-20T13:13:18.465775Z","shell.execute_reply.started":"2024-07-20T13:13:17.414276Z","shell.execute_reply":"2024-07-20T13:13:18.464932Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b493b037dee4c2ca2c0397b0e4c9070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b0c51917c8a47d99d5f1465b3f538d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd2629fba47540d9916035959e031080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230bcc3efaa642e3ada251c1a58749e0"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer(\"Using a Transformer network is simple\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.467362Z","iopub.execute_input":"2024-07-20T13:13:18.467719Z","iopub.status.idle":"2024-07-20T13:13:18.474711Z","shell.execute_reply.started":"2024-07-20T13:13:18.467684Z","shell.execute_reply":"2024-07-20T13:13:18.473536Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nsequence = \"Using a Transformer network is simple\"\ntokens = tokenizer.tokenize(sequence)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.475892Z","iopub.execute_input":"2024-07-20T13:13:18.476278Z","iopub.status.idle":"2024-07-20T13:13:18.605642Z","shell.execute_reply.started":"2024-07-20T13:13:18.476253Z","shell.execute_reply":"2024-07-20T13:13:18.604765Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n","output_type":"stream"}]},{"cell_type":"code","source":"ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.606917Z","iopub.execute_input":"2024-07-20T13:13:18.607307Z","iopub.status.idle":"2024-07-20T13:13:18.612211Z","shell.execute_reply.started":"2024-07-20T13:13:18.607271Z","shell.execute_reply":"2024-07-20T13:13:18.611286Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[7993, 170, 13809, 23763, 2443, 1110, 3014]\n","output_type":"stream"}]},{"cell_type":"code","source":"new = tokenizer.tokenize(\"Using a Transformer network is tough\")\nprint(new)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.613413Z","iopub.execute_input":"2024-07-20T13:13:18.613712Z","iopub.status.idle":"2024-07-20T13:13:18.623657Z","shell.execute_reply.started":"2024-07-20T13:13:18.613683Z","shell.execute_reply":"2024-07-20T13:13:18.622769Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"['Using', 'a', 'Trans', '##former', 'network', 'is', 'tough']\n","output_type":"stream"}]},{"cell_type":"code","source":"news = tokenizer.convert_tokens_to_ids(new)\nprint(news)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.624803Z","iopub.execute_input":"2024-07-20T13:13:18.625095Z","iopub.status.idle":"2024-07-20T13:13:18.634124Z","shell.execute_reply.started":"2024-07-20T13:13:18.625071Z","shell.execute_reply":"2024-07-20T13:13:18.633207Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[7993, 170, 13809, 23763, 2443, 1110, 8035]\n","output_type":"stream"}]},{"cell_type":"code","source":"decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\nprint(decoded_string)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.635055Z","iopub.execute_input":"2024-07-20T13:13:18.635333Z","iopub.status.idle":"2024-07-20T13:13:18.645152Z","shell.execute_reply.started":"2024-07-20T13:13:18.635309Z","shell.execute_reply":"2024-07-20T13:13:18.643814Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Using a transformer network is simple\n","output_type":"stream"}]},{"cell_type":"code","source":"decoded_string_2 = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 8035])\nprint(decoded_string_2)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.646253Z","iopub.execute_input":"2024-07-20T13:13:18.646488Z","iopub.status.idle":"2024-07-20T13:13:18.655452Z","shell.execute_reply.started":"2024-07-20T13:13:18.646468Z","shell.execute_reply":"2024-07-20T13:13:18.654647Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Using a transformer network is tough\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\nprint(tokenized_inputs[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.656923Z","iopub.execute_input":"2024-07-20T13:13:18.657252Z","iopub.status.idle":"2024-07-20T13:13:18.667318Z","shell.execute_reply.started":"2024-07-20T13:13:18.657223Z","shell.execute_reply":"2024-07-20T13:13:18.666513Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tensor([[  101,  7993,   170, 13809, 23763,  2443,  1110,  3014,   102]])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\n\ninput_ids = torch.tensor([ids])\nprint(\"Input IDs:\", input_ids)\n\noutput = model(input_ids)\nprint(\"Logits:\", output.logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:18.668497Z","iopub.execute_input":"2024-07-20T13:13:18.669229Z","iopub.status.idle":"2024-07-20T13:13:19.085963Z","shell.execute_reply.started":"2024-07-20T13:13:18.669203Z","shell.execute_reply":"2024-07-20T13:13:19.085032Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n          2026,  2878,  2166,  1012]])\nLogits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"padding_id = 100\n\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, padding_id],\n]\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence1_ids = [[200, 200, 200]]\nsequence2_ids = [[200, 200]]\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id],\n]\n\nprint(model(torch.tensor(sequence1_ids)).logits)\nprint(model(torch.tensor(sequence2_ids)).logits)\nprint(model(torch.tensor(batched_ids)).logits)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.087342Z","iopub.execute_input":"2024-07-20T13:13:19.087769Z","iopub.status.idle":"2024-07-20T13:13:19.331884Z","shell.execute_reply.started":"2024-07-20T13:13:19.087736Z","shell.execute_reply":"2024-07-20T13:13:19.330998Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","output_type":"stream"},{"name":"stdout","text":"tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\ntensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\ntensor([[ 1.5694, -1.3895],\n        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\nmodel_inputs = tokenizer(sequence)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.333278Z","iopub.execute_input":"2024-07-20T13:13:19.333720Z","iopub.status.idle":"2024-07-20T13:13:19.477145Z","shell.execute_reply.started":"2024-07-20T13:13:19.333685Z","shell.execute_reply":"2024-07-20T13:13:19.476387Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\nmodel_inputs = tokenizer(sequences)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.478300Z","iopub.execute_input":"2024-07-20T13:13:19.478646Z","iopub.status.idle":"2024-07-20T13:13:19.483650Z","shell.execute_reply.started":"2024-07-20T13:13:19.478613Z","shell.execute_reply":"2024-07-20T13:13:19.482554Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Will pad the sequences up to the maximum sequence length\nmodel_inputs = tokenizer(sequences, padding=\"longest\")\n\n# Will pad the sequences up to the model max length\n# (512 for BERT or DistilBERT)\nmodel_inputs = tokenizer(sequences, padding=\"max_length\")\n\n# Will pad the sequences up to the specified max length\nmodel_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.485226Z","iopub.execute_input":"2024-07-20T13:13:19.485653Z","iopub.status.idle":"2024-07-20T13:13:19.498257Z","shell.execute_reply.started":"2024-07-20T13:13:19.485616Z","shell.execute_reply":"2024-07-20T13:13:19.497526Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\n# Will truncate the sequences that are longer than the model max length\n# (512 for BERT or DistilBERT)\nmodel_inputs = tokenizer(sequences, truncation=True)\n\n# Will truncate the sequences that are longer than the specified max length\nmodel_inputs = tokenizer(sequences, max_length=8, truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.504149Z","iopub.execute_input":"2024-07-20T13:13:19.504391Z","iopub.status.idle":"2024-07-20T13:13:19.509544Z","shell.execute_reply.started":"2024-07-20T13:13:19.504370Z","shell.execute_reply":"2024-07-20T13:13:19.508706Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\nmodel_inputs = tokenizer(sequence)\nprint(model_inputs[\"input_ids\"])\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.510731Z","iopub.execute_input":"2024-07-20T13:13:19.511344Z","iopub.status.idle":"2024-07-20T13:13:19.519110Z","shell.execute_reply.started":"2024-07-20T13:13:19.511309Z","shell.execute_reply":"2024-07-20T13:13:19.518285Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.520237Z","iopub.execute_input":"2024-07-20T13:13:19.520666Z","iopub.status.idle":"2024-07-20T13:13:19.928091Z","shell.execute_reply.started":"2024-07-20T13:13:19.520634Z","shell.execute_reply":"2024-07-20T13:13:19.927256Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# **Chapter 3 **","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n\n# Same as before\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"This course is amazing!\",\n]\nbatch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n\n# This is new\nbatch[\"labels\"] = torch.tensor([1, 1])\n\noptimizer = AdamW(model.parameters())\nloss = model(**batch).loss\nloss.backward()\noptimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:19.929349Z","iopub.execute_input":"2024-07-20T13:13:19.929704Z","iopub.status.idle":"2024-07-20T13:13:21.315205Z","shell.execute_reply.started":"2024-07-20T13:13:19.929669Z","shell.execute_reply":"2024-07-20T13:13:21.314335Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:21.316449Z","iopub.execute_input":"2024-07-20T13:13:21.316793Z","iopub.status.idle":"2024-07-20T13:13:29.191617Z","shell.execute_reply.started":"2024-07-20T13:13:21.316761Z","shell.execute_reply":"2024-07-20T13:13:29.190792Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2200109e54941b9b268f491af53f50b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d42748c727412090e2b1330e605f1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df34b202126b49b184c6001d5ebda0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5c269d764d343789a45612b371ea043"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abffb52104b9436ba24d6c22317deee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e112770721de4729bababc84c010b6fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b57052cb6ab41a29bb875848e5560df"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"raw_train_dataset = raw_datasets[\"train\"]\nraw_train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:29.192733Z","iopub.execute_input":"2024-07-20T13:13:29.193278Z","iopub.status.idle":"2024-07-20T13:13:29.203883Z","shell.execute_reply.started":"2024-07-20T13:13:29.193251Z","shell.execute_reply":"2024-07-20T13:13:29.202563Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n 'label': 1,\n 'idx': 0}"},"metadata":{}}]},{"cell_type":"code","source":"raw_train_dataset = raw_datasets[\"train\"]\nraw_test_dataset = raw_datasets[\"test\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:29.205098Z","iopub.execute_input":"2024-07-20T13:13:29.205378Z","iopub.status.idle":"2024-07-20T13:13:29.216275Z","shell.execute_reply.started":"2024-07-20T13:13:29.205354Z","shell.execute_reply":"2024-07-20T13:13:29.215296Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\ntokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:29.217456Z","iopub.execute_input":"2024-07-20T13:13:29.217743Z","iopub.status.idle":"2024-07-20T13:13:30.052011Z","shell.execute_reply.started":"2024-07-20T13:13:29.217710Z","shell.execute_reply":"2024-07-20T13:13:30.051232Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\ninputs","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:30.053029Z","iopub.execute_input":"2024-07-20T13:13:30.053283Z","iopub.status.idle":"2024-07-20T13:13:30.059459Z","shell.execute_reply.started":"2024-07-20T13:13:30.053260Z","shell.execute_reply":"2024-07-20T13:13:30.058564Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:30.060392Z","iopub.execute_input":"2024-07-20T13:13:30.060638Z","iopub.status.idle":"2024-07-20T13:13:30.071003Z","shell.execute_reply.started":"2024-07-20T13:13:30.060616Z","shell.execute_reply":"2024-07-20T13:13:30.070151Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'this',\n 'is',\n 'the',\n 'first',\n 'sentence',\n '.',\n '[SEP]',\n 'this',\n 'is',\n 'the',\n 'second',\n 'one',\n '.',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset = tokenizer(\n    raw_datasets[\"train\"][\"sentence1\"],\n    raw_datasets[\"train\"][\"sentence2\"],\n    padding=True,\n    truncation=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:30.072036Z","iopub.execute_input":"2024-07-20T13:13:30.072284Z","iopub.status.idle":"2024-07-20T13:13:30.794337Z","shell.execute_reply.started":"2024-07-20T13:13:30.072262Z","shell.execute_reply":"2024-07-20T13:13:30.793552Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:30.795436Z","iopub.execute_input":"2024-07-20T13:13:30.795732Z","iopub.status.idle":"2024-07-20T13:13:30.800591Z","shell.execute_reply.started":"2024-07-20T13:13:30.795706Z","shell.execute_reply":"2024-07-20T13:13:30.799557Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:30.801835Z","iopub.execute_input":"2024-07-20T13:13:30.802275Z","iopub.status.idle":"2024-07-20T13:13:32.215702Z","shell.execute_reply.started":"2024-07-20T13:13:30.802243Z","shell.execute_reply":"2024-07-20T13:13:32.214805Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798e2cae5c6a43939cb4b36a190f6eb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f72bac4bf645fa92eafe0afc9934f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"497324ced6124d638fb497f856ec769b"}},"metadata":{}},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1725\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:32.216915Z","iopub.execute_input":"2024-07-20T13:13:32.217280Z","iopub.status.idle":"2024-07-20T13:13:32.221782Z","shell.execute_reply.started":"2024-07-20T13:13:32.217249Z","shell.execute_reply":"2024-07-20T13:13:32.220931Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"samples = tokenized_datasets[\"train\"][:8]\nsamples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n[len(x) for x in samples[\"input_ids\"]]","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:32.223220Z","iopub.execute_input":"2024-07-20T13:13:32.223517Z","iopub.status.idle":"2024-07-20T13:13:32.244642Z","shell.execute_reply.started":"2024-07-20T13:13:32.223494Z","shell.execute_reply":"2024-07-20T13:13:32.243663Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[50, 59, 47, 67, 59, 50, 62, 32]"},"metadata":{}}]},{"cell_type":"code","source":"batch = data_collator(samples)\n{k: v.shape for k, v in batch.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:32.245638Z","iopub.execute_input":"2024-07-20T13:13:32.245881Z","iopub.status.idle":"2024-07-20T13:13:32.256690Z","shell.execute_reply.started":"2024-07-20T13:13:32.245858Z","shell.execute_reply":"2024-07-20T13:13:32.255931Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{'input_ids': torch.Size([8, 67]),\n 'token_type_ids': torch.Size([8, 67]),\n 'attention_mask': torch.Size([8, 67]),\n 'labels': torch.Size([8])}"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:32.257730Z","iopub.execute_input":"2024-07-20T13:13:32.258002Z","iopub.status.idle":"2024-07-20T13:13:35.957211Z","shell.execute_reply.started":"2024-07-20T13:13:32.257963Z","shell.execute_reply":"2024-07-20T13:13:35.956274Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6effc20e75c471c900b430d4ef9e562"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Training using Trainer api","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\ntraining_args = TrainingArguments(\"test-trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:35.958508Z","iopub.execute_input":"2024-07-20T13:13:35.959213Z","iopub.status.idle":"2024-07-20T13:13:36.031311Z","shell.execute_reply.started":"2024-07-20T13:13:35.959176Z","shell.execute_reply":"2024-07-20T13:13:36.030376Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:36.032583Z","iopub.execute_input":"2024-07-20T13:13:36.033210Z","iopub.status.idle":"2024-07-20T13:13:36.374110Z","shell.execute_reply.started":"2024-07-20T13:13:36.033170Z","shell.execute_reply":"2024-07-20T13:13:36.373362Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:36.375280Z","iopub.execute_input":"2024-07-20T13:13:36.375539Z","iopub.status.idle":"2024-07-20T13:13:38.102857Z","shell.execute_reply.started":"2024-07-20T13:13:36.375515Z","shell.execute_reply":"2024-07-20T13:13:38.101917Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n#d8ded67bb5d5d9b9a148342319df7f0ab97ef45d","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:13:38.104166Z","iopub.execute_input":"2024-07-20T13:13:38.104797Z","iopub.status.idle":"2024-07-20T13:19:34.487902Z","shell.execute_reply.started":"2024-07-20T13:13:38.104761Z","shell.execute_reply":"2024-07-20T13:19:34.487022Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240720_131638-tjko0uyp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/arusharmazxx000-uiet/huggingface/runs/tjko0uyp' target=\"_blank\">test-trainer</a></strong> to <a href='https://wandb.ai/arusharmazxx000-uiet/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/arusharmazxx000-uiet/huggingface' target=\"_blank\">https://wandb.ai/arusharmazxx000-uiet/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/arusharmazxx000-uiet/huggingface/runs/tjko0uyp' target=\"_blank\">https://wandb.ai/arusharmazxx000-uiet/huggingface/runs/tjko0uyp</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='690' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [690/690 02:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.399800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=690, training_loss=0.3289282480875651, metrics={'train_runtime': 355.7973, 'train_samples_per_second': 30.928, 'train_steps_per_second': 1.939, 'total_flos': 428577075854640.0, 'train_loss': 0.3289282480875651, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"validation\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:19:34.489257Z","iopub.execute_input":"2024-07-20T13:19:34.489550Z","iopub.status.idle":"2024-07-20T13:19:36.458873Z","shell.execute_reply.started":"2024-07-20T13:19:34.489526Z","shell.execute_reply":"2024-07-20T13:19:36.458033Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"(408, 2) (408,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\npreds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:21:35.559721Z","iopub.execute_input":"2024-07-20T13:21:35.560170Z","iopub.status.idle":"2024-07-20T13:21:35.566690Z","shell.execute_reply.started":"2024-07-20T13:21:35.560139Z","shell.execute_reply":"2024-07-20T13:21:35.565716Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#!pip install evaluate\nimport evaluate\nmetric = evaluate.load(\"glue\", \"mrpc\")\nmetric.compute(predictions=preds, references=predictions.label_ids)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:22:20.006285Z","iopub.execute_input":"2024-07-20T13:22:20.006935Z","iopub.status.idle":"2024-07-20T13:22:34.963838Z","shell.execute_reply.started":"2024-07-20T13:22:20.006900Z","shell.execute_reply":"2024-07-20T13:22:34.962477Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m990.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1112226b11234fbc9b1550c680edbfa7"}},"metadata":{}},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.8308823529411765, 'f1': 0.8828522920203735}"},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    metric = evaluate.load(\"glue\", \"mrpc\")\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:23:20.564841Z","iopub.execute_input":"2024-07-20T13:23:20.565525Z","iopub.status.idle":"2024-07-20T13:23:20.571921Z","shell.execute_reply.started":"2024-07-20T13:23:20.565487Z","shell.execute_reply":"2024-07-20T13:23:20.570868Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:23:21.899818Z","iopub.execute_input":"2024-07-20T13:23:21.900233Z","iopub.status.idle":"2024-07-20T13:23:22.536852Z","shell.execute_reply.started":"2024-07-20T13:23:21.900200Z","shell.execute_reply":"2024-07-20T13:23:22.535782Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:23:25.067818Z","iopub.execute_input":"2024-07-20T13:23:25.068448Z","iopub.status.idle":"2024-07-20T13:26:11.587734Z","shell.execute_reply.started":"2024-07-20T13:23:25.068413Z","shell.execute_reply":"2024-07-20T13:26:11.586841Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='690' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [690/690 02:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.341154</td>\n      <td>0.860294</td>\n      <td>0.902564</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.403555</td>\n      <td>0.860294</td>\n      <td>0.905473</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.379900</td>\n      <td>0.611330</td>\n      <td>0.867647</td>\n      <td>0.910299</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=690, training_loss=0.31263936360677086, metrics={'train_runtime': 165.925, 'train_samples_per_second': 66.319, 'train_steps_per_second': 4.159, 'total_flos': 428577075854640.0, 'train_loss': 0.31263936360677086, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Pytorch Edition of Training Loop","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, padding=\"max_length\")\n\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True, )\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:04:51.502602Z","iopub.execute_input":"2024-07-20T14:04:51.503201Z","iopub.status.idle":"2024-07-20T14:04:57.469410Z","shell.execute_reply.started":"2024-07-20T14:04:51.503167Z","shell.execute_reply":"2024-07-20T14:04:57.468255Z"},"trusted":true},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9879862b378e4cf3b1e564586151a443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1be7281f3a04739af3d1764b7160f60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65266f6d1ec9428884ba8d957465209a"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:03.239760Z","iopub.execute_input":"2024-07-20T14:05:03.240211Z","iopub.status.idle":"2024-07-20T14:05:03.264208Z","shell.execute_reply.started":"2024-07-20T14:05:03.240179Z","shell.execute_reply":"2024-07-20T14:05:03.263380Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"['labels', 'input_ids', 'token_type_ids', 'attention_mask']"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:06.851508Z","iopub.execute_input":"2024-07-20T14:05:06.851868Z","iopub.status.idle":"2024-07-20T14:05:06.859014Z","shell.execute_reply.started":"2024-07-20T14:05:06.851841Z","shell.execute_reply":"2024-07-20T14:05:06.857812Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"for batch in train_dataloader:\n    break\n{k: v.shape for k, v in batch.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:09.371628Z","iopub.execute_input":"2024-07-20T14:05:09.372325Z","iopub.status.idle":"2024-07-20T14:05:09.390840Z","shell.execute_reply.started":"2024-07-20T14:05:09.372274Z","shell.execute_reply":"2024-07-20T14:05:09.389854Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"{'labels': torch.Size([8]),\n 'input_ids': torch.Size([8, 512]),\n 'token_type_ids': torch.Size([8, 512]),\n 'attention_mask': torch.Size([8, 512])}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:16.129744Z","iopub.execute_input":"2024-07-20T14:05:16.130541Z","iopub.status.idle":"2024-07-20T14:05:16.390843Z","shell.execute_reply.started":"2024-07-20T14:05:16.130504Z","shell.execute_reply":"2024-07-20T14:05:16.390124Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"outputs = model(**batch)\nprint(outputs.loss, outputs.logits.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:17.382965Z","iopub.execute_input":"2024-07-20T14:05:17.383339Z","iopub.status.idle":"2024-07-20T14:05:22.840589Z","shell.execute_reply.started":"2024-07-20T14:05:17.383310Z","shell.execute_reply":"2024-07-20T14:05:22.839535Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"tensor(0.9389, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:22.842138Z","iopub.execute_input":"2024-07-20T14:05:22.842420Z","iopub.status.idle":"2024-07-20T14:05:22.851060Z","shell.execute_reply.started":"2024-07-20T14:05:22.842395Z","shell.execute_reply":"2024-07-20T14:05:22.850317Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"from transformers import get_scheduler\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\nprint(num_training_steps)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:23.085089Z","iopub.execute_input":"2024-07-20T14:05:23.085385Z","iopub.status.idle":"2024-07-20T14:05:23.093343Z","shell.execute_reply.started":"2024-07-20T14:05:23.085361Z","shell.execute_reply":"2024-07-20T14:05:23.091875Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"1377\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:24.332293Z","iopub.execute_input":"2024-07-20T14:05:24.333151Z","iopub.status.idle":"2024-07-20T14:05:24.450724Z","shell.execute_reply.started":"2024-07-20T14:05:24.333117Z","shell.execute_reply":"2024-07-20T14:05:24.449734Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:50:07.655794Z","iopub.execute_input":"2024-07-20T13:50:07.656162Z","iopub.status.idle":"2024-07-20T13:52:57.348940Z","shell.execute_reply.started":"2024-07-20T13:50:07.656134Z","shell.execute_reply":"2024-07-20T13:52:57.347941Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108b29c0bc434648a03bd964ac111922"}},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"glue\", \"mrpc\")\nmodel.eval()\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:53:38.878195Z","iopub.execute_input":"2024-07-20T13:53:38.879159Z","iopub.status.idle":"2024-07-20T13:53:41.297751Z","shell.execute_reply.started":"2024-07-20T13:53:38.879120Z","shell.execute_reply":"2024-07-20T13:53:41.296630Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.8504901960784313, 'f1': 0.896434634974533}"},"metadata":{}}]},{"cell_type":"markdown","source":"***Summarized here***\nfor accelerated training","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\ntrain_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(train_dataloader, eval_dataloader, model, optimizer)\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        #batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:05:32.525563Z","iopub.execute_input":"2024-07-20T14:05:32.526186Z","iopub.status.idle":"2024-07-20T14:22:35.355147Z","shell.execute_reply.started":"2024-07-20T14:05:32.526152Z","shell.execute_reply":"2024-07-20T14:22:35.353903Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d57a6a6e83e24223a0d4c82c907d1fce"}},"metadata":{}}]},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"glue\", \"mrpc\")\nmodel.eval()\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T14:28:41.933515Z","iopub.execute_input":"2024-07-20T14:28:41.933863Z","iopub.status.idle":"2024-07-20T14:28:53.600119Z","shell.execute_reply.started":"2024-07-20T14:28:41.933837Z","shell.execute_reply":"2024-07-20T14:28:53.599161Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.8602941176470589, 'f1': 0.9015544041450777}"},"metadata":{}}]}]}